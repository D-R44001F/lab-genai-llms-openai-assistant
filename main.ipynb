{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB GenAI - LLMs - OpenAI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import fitz \n",
    "import openai\n",
    "import requests\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an assistant to answer a topic of your choosing:\n",
    " - Upload a file of your interest\n",
    " - Add Instructions to the prompt\n",
    " - Use the assistant in Playground mode\n",
    "\n",
    " https://platform.openai.com/playground/assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_20784\\1030107297.py:45: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm=ChatOpenAI(model_name=\"gpt-4\", temperature=0),\n",
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_20784\\1030107297.py:51: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(user_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ohm’s Law is an equation that allows you to calculate the relationship between voltage, current, and resistance in an electrical circuit. It states that the voltage equals current multiplied by resistance, or in standard mathematical notation V = I x R. This means if you know any two of the three values in the equation, you can calculate the third.\n"
     ]
    }
   ],
   "source": [
    "# Read OpenAI API key from file\n",
    "with open(\"API_Key.txt\", \"r\") as file:\n",
    "    openai_api_key = file.read().strip()  # Remove extra spaces/newlines\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key  # Required for LangChain\n",
    "\n",
    "# Now, initialize OpenAI embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)  # Pass explicitly\n",
    "\n",
    "# Step 1: Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Step 2: Load PDF and split text into chunks\n",
    "pdf_path = \"Electronics for Dummies.pdf\"\n",
    "text_data = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Split into 1000-character chunks\n",
    "    chunk_overlap=100  # Ensure context between chunks\n",
    ")\n",
    "chunks = text_splitter.split_text(text_data)\n",
    "\n",
    "# Step 3: Create vector store (ChromaDB) using OpenAI embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "vector_store = Chroma.from_texts(chunks, embedding)\n",
    "\n",
    "# Step 4: Set up Retrieval-Augmented Generation (RAG)\n",
    "retriever = vector_store.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4\", temperature=0),\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# Step 5: User Query\n",
    "user_query = \"What is Ohm's Law?\"\n",
    "response = qa_chain.run(user_query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk to your assistant via the API\n",
    "\n",
    "https://platform.openai.com/docs/assistants/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics Assistant (type 'exit' to stop)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Hello, what is a transistor?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_20784\\1288566343.py:41: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"question\": user_query, \"chat_history\": chat_history})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: A transistor is a device that controls the flow of electric current by opening and closing a kind of valve within it. It can be used as either a switch or an amplifier. A transistor has three leads: Base, emitter, and collector. When used as a switch, the base lead of the transistor works like the toggle on a mechanical switch. Without transistors, many modern electronic devices like radios, cell phones, and computers would be much larger.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Load API Key from .env\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Step 1: Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Load PDF and process it\n",
    "pdf_path = \"Electronics for Dummies.pdf\"\n",
    "text_data = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Step 2: Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_text(text_data)\n",
    "\n",
    "# Step 3: Create vector store with OpenAI embeddings\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vector_store = Chroma.from_texts(chunks, embedding)\n",
    "\n",
    "# Step 4: Set up Conversational Retrieval Chain (keeps context)\n",
    "retriever = vector_store.as_retriever()\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(model_name=\"gpt-4\", temperature=0),\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# Step 5: Conversation loop\n",
    "chat_history = []  # Stores previous interactions\n",
    "print(\"Electronics Assistant (type 'exit' to stop)\")\n",
    "while True:\n",
    "    user_query = input(\"\\nYou: \")\n",
    "    if user_query.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    response = qa_chain({\"question\": user_query, \"chat_history\": chat_history})\n",
    "    chat_history.append((user_query, response[\"answer\"]))  # Keep history\n",
    "    \n",
    "    print(\"\\nAssistant:\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an assistant that will call a weather API, given the user's answer and return the proper answer.\n",
    "\n",
    "See the documentation of the weather API here: https://open-meteo.com/en/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latitude': 52.52, 'longitude': 13.419998, 'generationtime_ms': 0.06008148193359375, 'utc_offset_seconds': 0, 'timezone': 'GMT', 'timezone_abbreviation': 'GMT', 'elevation': 38.0, 'hourly_units': {'time': 'iso8601', 'temperature_2m': '°C'}, 'hourly': {'time': ['2025-02-04T00:00', '2025-02-04T01:00', '2025-02-04T02:00', '2025-02-04T03:00', '2025-02-04T04:00', '2025-02-04T05:00', '2025-02-04T06:00', '2025-02-04T07:00', '2025-02-04T08:00', '2025-02-04T09:00', '2025-02-04T10:00', '2025-02-04T11:00', '2025-02-04T12:00', '2025-02-04T13:00', '2025-02-04T14:00', '2025-02-04T15:00', '2025-02-04T16:00', '2025-02-04T17:00', '2025-02-04T18:00', '2025-02-04T19:00', '2025-02-04T20:00', '2025-02-04T21:00', '2025-02-04T22:00', '2025-02-04T23:00', '2025-02-05T00:00', '2025-02-05T01:00', '2025-02-05T02:00', '2025-02-05T03:00', '2025-02-05T04:00', '2025-02-05T05:00', '2025-02-05T06:00', '2025-02-05T07:00', '2025-02-05T08:00', '2025-02-05T09:00', '2025-02-05T10:00', '2025-02-05T11:00', '2025-02-05T12:00', '2025-02-05T13:00', '2025-02-05T14:00', '2025-02-05T15:00', '2025-02-05T16:00', '2025-02-05T17:00', '2025-02-05T18:00', '2025-02-05T19:00', '2025-02-05T20:00', '2025-02-05T21:00', '2025-02-05T22:00', '2025-02-05T23:00', '2025-02-06T00:00', '2025-02-06T01:00', '2025-02-06T02:00', '2025-02-06T03:00', '2025-02-06T04:00', '2025-02-06T05:00', '2025-02-06T06:00', '2025-02-06T07:00', '2025-02-06T08:00', '2025-02-06T09:00', '2025-02-06T10:00', '2025-02-06T11:00', '2025-02-06T12:00', '2025-02-06T13:00', '2025-02-06T14:00', '2025-02-06T15:00', '2025-02-06T16:00', '2025-02-06T17:00', '2025-02-06T18:00', '2025-02-06T19:00', '2025-02-06T20:00', '2025-02-06T21:00', '2025-02-06T22:00', '2025-02-06T23:00', '2025-02-07T00:00', '2025-02-07T01:00', '2025-02-07T02:00', '2025-02-07T03:00', '2025-02-07T04:00', '2025-02-07T05:00', '2025-02-07T06:00', '2025-02-07T07:00', '2025-02-07T08:00', '2025-02-07T09:00', '2025-02-07T10:00', '2025-02-07T11:00', '2025-02-07T12:00', '2025-02-07T13:00', '2025-02-07T14:00', '2025-02-07T15:00', '2025-02-07T16:00', '2025-02-07T17:00', '2025-02-07T18:00', '2025-02-07T19:00', '2025-02-07T20:00', '2025-02-07T21:00', '2025-02-07T22:00', '2025-02-07T23:00', '2025-02-08T00:00', '2025-02-08T01:00', '2025-02-08T02:00', '2025-02-08T03:00', '2025-02-08T04:00', '2025-02-08T05:00', '2025-02-08T06:00', '2025-02-08T07:00', '2025-02-08T08:00', '2025-02-08T09:00', '2025-02-08T10:00', '2025-02-08T11:00', '2025-02-08T12:00', '2025-02-08T13:00', '2025-02-08T14:00', '2025-02-08T15:00', '2025-02-08T16:00', '2025-02-08T17:00', '2025-02-08T18:00', '2025-02-08T19:00', '2025-02-08T20:00', '2025-02-08T21:00', '2025-02-08T22:00', '2025-02-08T23:00', '2025-02-09T00:00', '2025-02-09T01:00', '2025-02-09T02:00', '2025-02-09T03:00', '2025-02-09T04:00', '2025-02-09T05:00', '2025-02-09T06:00', '2025-02-09T07:00', '2025-02-09T08:00', '2025-02-09T09:00', '2025-02-09T10:00', '2025-02-09T11:00', '2025-02-09T12:00', '2025-02-09T13:00', '2025-02-09T14:00', '2025-02-09T15:00', '2025-02-09T16:00', '2025-02-09T17:00', '2025-02-09T18:00', '2025-02-09T19:00', '2025-02-09T20:00', '2025-02-09T21:00', '2025-02-09T22:00', '2025-02-09T23:00', '2025-02-10T00:00', '2025-02-10T01:00', '2025-02-10T02:00', '2025-02-10T03:00', '2025-02-10T04:00', '2025-02-10T05:00', '2025-02-10T06:00', '2025-02-10T07:00', '2025-02-10T08:00', '2025-02-10T09:00', '2025-02-10T10:00', '2025-02-10T11:00', '2025-02-10T12:00', '2025-02-10T13:00', '2025-02-10T14:00', '2025-02-10T15:00', '2025-02-10T16:00', '2025-02-10T17:00', '2025-02-10T18:00', '2025-02-10T19:00', '2025-02-10T20:00', '2025-02-10T21:00', '2025-02-10T22:00', '2025-02-10T23:00'], 'temperature_2m': [-1.9, -2.1, -2.3, -2.8, -3.1, -3.3, -3.5, -3.9, -3.7, -1.8, -0.4, 1.0, 3.2, 4.1, 4.2, 3.7, 2.2, 1.5, 0.9, 0.3, -0.1, -0.6, -0.9, -1.2, -1.7, -2.1, -2.5, -2.6, -2.8, -2.8, -2.8, -2.7, -1.8, -0.3, 1.0, 1.8, 2.0, 2.4, 3.1, 3.2, 2.6, 2.2, 2.2, 1.9, 1.9, 1.9, 1.8, 1.6, 1.6, 1.5, 1.4, 1.3, 1.2, 0.8, 0.6, 0.6, 0.9, 1.3, 2.3, 3.4, 4.8, 5.7, 5.7, 5.4, 4.8, 4.1, 3.6, 3.3, 2.9, 2.4, 1.9, 1.2, 0.5, 0.1, 0.5, 1.0, 1.1, 0.7, 0.7, 0.7, 1.2, 1.8, 2.7, 4.0, 4.3, 4.2, 4.4, 3.8, 3.1, 2.2, 1.2, 0.6, 0.2, -0.2, -0.6, -0.9, -1.1, -1.3, -1.3, -1.4, -1.7, -1.9, -2.0, -1.9, -1.6, -1.1, 0.0, 1.4, 2.4, 2.8, 2.9, 2.7, 2.2, 1.5, 0.8, 0.3, -0.1, -0.5, -0.7, -0.9, -1.1, -1.3, -1.6, -1.8, -2.2, -2.6, -2.7, -2.6, -2.2, -1.6, -0.4, 1.0, 2.1, 2.7, 2.8, 2.7, 2.0, 0.9, 0.1, -0.4, -0.7, -0.9, -1.1, -1.2, -1.4, -1.6, -1.9, -2.1, -2.1, -2.1, -2.1, -2.0, -1.8, -1.5, -0.9, -0.1, 0.5, 0.8, 0.9, 0.8, 0.5, 0.1, -0.4, -0.8, -1.1, -1.5, -1.9, -2.4]}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather_forecast(latitude, longitude):\n",
    "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": \"temperature_2m\"\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "# Example usage:\n",
    "forecast = get_weather_forecast(52.52, 13.41)\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: None\n"
     ]
    }
   ],
   "source": [
    "print(\"API Key:\", weather_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a city name (or type 'exit' to stop):  Washington\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Washington is Clear sky with a temperature of 13.8°C.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a city name (or type 'exit' to stop):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Function to get weather info using Open-Meteo\n",
    "def get_weather(city):\n",
    "    # Open-Meteo coordinates\n",
    "    city_coords = {\n",
    "        \"Washington\": {\"lat\": 38.9072, \"lon\": -77.0369},\n",
    "        \"Denver\": {\"lat\": 39.7392, \"lon\": -104.9903},\n",
    "        \"Paris\": {\"lat\": 48.8566, \"lon\": 2.3522},\n",
    "        \"London\": {\"lat\": 51.5074, \"lon\": -0.1278},\n",
    "    }\n",
    "    \n",
    "    # Check if city is in our predefined coordinates\n",
    "    if city not in city_coords:\n",
    "        return \"City not found in database.\"\n",
    "    \n",
    "    coords = city_coords[city]\n",
    "    lat, lon = coords[\"lat\"], coords[\"lon\"]\n",
    "    \n",
    "    # Open-Meteo API URL\n",
    "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"current_weather\": \"true\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        weather_code = data[\"current_weather\"][\"weathercode\"]\n",
    "        temperature = data[\"current_weather\"][\"temperature\"]\n",
    "        \n",
    "        # Mapping weather codes to human-readable descriptions\n",
    "        weather_conditions = {\n",
    "            0: \"Clear sky\",\n",
    "            1: \"Mainly clear\",\n",
    "            2: \"Partly cloudy\",\n",
    "            3: \"Cloudy\",\n",
    "            4: \"Overcast\",\n",
    "            5: \"Showers\",\n",
    "            6: \"Heavy showers\",\n",
    "            7: \"Thunderstorms\",\n",
    "            8: \"Heavy thunderstorms\",\n",
    "            9: \"Snow\"\n",
    "        }\n",
    "        \n",
    "        weather = weather_conditions.get(weather_code, \"Unknown weather condition\")\n",
    "        return f\"The weather in {city} is {weather} with a temperature of {temperature}°C.\"\n",
    "    else:\n",
    "        return \"Error: Couldn't retrieve weather data.\"\n",
    "\n",
    "# Simple interaction\n",
    "while True:\n",
    "    city = input(\"\\nEnter a city name (or type 'exit' to stop): \")\n",
    "    if city.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    print(get_weather(city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to, there is a hint here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Chatbots / Assistants have a way to respond in json format. \n",
    "\n",
    "Explore the function calling functionality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
